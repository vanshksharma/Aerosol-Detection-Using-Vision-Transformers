{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import tensorflow_probability as tfp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "tf.keras.backend.set_image_data_format('channels_first')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "import keras.layers\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "import tensorflow_probability as tfp\n",
    "from utils import to_2tuple\n",
    "\n",
    "np_config.enable_numpy_behavior()\n",
    "\n",
    "\n",
    "def droppath(x, drop_prob: float = 0., training: bool = False, scale_by_keep: bool = True):\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "\n",
    "    bernoulli_dist = tfp.distributions.Bernoulli(probs=keep_prob)\n",
    "    shape = (x.shape[0],) + (1,) * (tf.rank(x).numpy() - 1)\n",
    "    random_tensor = bernoulli_dist.sample(sample_shape=shape)\n",
    "\n",
    "    if keep_prob > 0.0 and scale_by_keep:\n",
    "        tf.divide(random_tensor, keep_prob)\n",
    "\n",
    "    return x * random_tensor\n",
    "\n",
    "\n",
    "class DropPath(keras.layers.Layer):\n",
    "    def __init__(self, drop_prob: float = 0., scale_by_keep: bool = True):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.scale_by_keep = scale_by_keep\n",
    "\n",
    "    def call(self, x, **kwargs):\n",
    "        return droppath(x, self.drop_prob, self.training, self.scale_by_keep)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f'drop_prob={round(self.drop_prob, 3):0.3f}'\n",
    "\n",
    "\n",
    "class Mlp(keras.layers.Layer):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=tf.nn.gelu, bias=True, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        bias = to_2tuple(bias)\n",
    "        drop_probs = to_2tuple(drop)\n",
    "\n",
    "        self.fc1 = keras.layers.Dense(hidden_features, use_bias=bias[0])\n",
    "        self.act = act_layer\n",
    "        self.drop1 = keras.layers.Dropout(drop_probs[0])\n",
    "        self.fc2 = keras.layers.Dense(out_features, use_bias=bias[1])\n",
    "        self.drop2 = keras.layers.Dropout(drop_probs[1])\n",
    "\n",
    "    def call(self, x, **kwargs):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Identity(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def call(self, x, **kwargs):\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(keras.layers.Layer):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = keras.layers.Dense(dim * 3, use_bias=qkv_bias)\n",
    "        self.attn_drop = keras.layers.Dropout(attn_drop)\n",
    "        self.proj = keras.layers.Dense(dim)\n",
    "        self.proj_drop = keras.layers.Dropout(proj_drop)\n",
    "\n",
    "    def call(self, x, **kwargs):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).transpose((2, 0, 3, 1, 4))\n",
    "        q, k, v = tf.unstack(qkv, axis=0)\n",
    "\n",
    "        attn = (q @ tf.experimental.numpy.swapaxes(k, -2, -1)) * self.scale\n",
    "        attn = tf.nn.softmax(attn, axis=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = tf.experimental.numpy.swapaxes((attn @ v), 1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class LayerScale(keras.layers.Layer):\n",
    "    def __init__(self, dim, init_values=1e-5, inplace=False):\n",
    "        super().__init__()\n",
    "        self.inplace = inplace\n",
    "        self.gamma = tf.Variable(initial_value=init_values * tf.ones(dim), trainable=True)\n",
    "\n",
    "    def call(self, x, **kwargs):\n",
    "        if self.inplace:\n",
    "            x = x * self.gamma\n",
    "            return x\n",
    "\n",
    "        return x * self.gamma\n",
    "\n",
    "\n",
    "class Block(keras.layers.Layer):\n",
    "    def __init__(\n",
    "            self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0., init_values=None,\n",
    "            drop_path=0., act_layer=tf.nn.gelu, norm_layer=keras.layers.LayerNormalization):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer()\n",
    "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.ls1 = LayerScale(dim, init_values=init_values) if init_values else Identity()\n",
    "        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else Identity()\n",
    "\n",
    "        self.norm2 = norm_layer()\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=drop)\n",
    "        self.ls2 = LayerScale(dim, init_values=init_values) if init_values else Identity()\n",
    "        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else Identity()\n",
    "\n",
    "    def call(self, x, **kwargs):\n",
    "        x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))\n",
    "        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "\n",
    "from utils import to_2tuple\n",
    "\n",
    "np_config.enable_numpy_behavior()\n",
    "\n",
    "\n",
    "# INITIALIZER=keras.initializers.TruncatedNormal(stddev=0.02)\n",
    "\n",
    "\n",
    "class PatchEmbed(keras.layers.Layer):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, multi_conv=False):\n",
    "        super().__init__()\n",
    "        self.in_chans = in_chans\n",
    "        img_size = (img_size, img_size)\n",
    "        patch_size = (patch_size, patch_size)\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        if multi_conv:\n",
    "            if patch_size[0] == 12:\n",
    "                self.proj = keras.models.Sequential([\n",
    "                    keras.layers.ZeroPadding2D(padding=3),\n",
    "                    keras.layers.Conv2D(embed_dim // 4, kernel_size=7, strides=4, activation=\"relu\"),\n",
    "                    keras.layers.Conv2D(embed_dim // 2, kernel_size=3, strides=3, activation=\"relu\"),\n",
    "                    keras.layers.ZeroPadding2D(padding=1),\n",
    "                    keras.layers.Conv2D(embed_dim, kernel_size=3, strides=1, activation=None),\n",
    "                ])\n",
    "\n",
    "            elif patch_size[0] == 16:\n",
    "                self.proj = keras.models.Sequential([\n",
    "                    keras.layers.ZeroPadding2D(padding=3),\n",
    "                    keras.layers.Conv2D(embed_dim // 4, kernel_size=7, strides=4, activation=\"relu\"),\n",
    "                    keras.layers.ZeroPadding2D(padding=1),\n",
    "                    keras.layers.Conv2D(embed_dim // 2, kernel_size=3, strides=2, activation=\"relu\"),\n",
    "                    keras.layers.ZeroPadding2D(padding=1),\n",
    "                    keras.layers.Conv2D(embed_dim, kernel_size=3, strides=2, activation=None),\n",
    "                ])\n",
    "\n",
    "        else:\n",
    "            self.proj = keras.layers.Conv2D(embed_dim, kernel_size=patch_size, strides=patch_size, activation=None)\n",
    "\n",
    "    def call(self, x):\n",
    "\n",
    "        B, C, H, W = x.shape\n",
    "        assert H == self.img_size[0] and W == self.img_size[\n",
    "            1], f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x)\n",
    "        x = tf.reshape(x,shape=(x.shape[0],x.shape[1],-1))\n",
    "        x=tf.experimental.numpy.swapaxes(x,1,2)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class CrossAttention(keras.layers.Layer):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        self.wq = keras.layers.Dense(dim, use_bias=qkv_bias)\n",
    "        self.wk = keras.layers.Dense(dim, use_bias=qkv_bias)\n",
    "        self.wv = keras.layers.Dense(dim, use_bias=qkv_bias)\n",
    "        self.attn_drop = keras.layers.Dropout(attn_drop)\n",
    "        self.proj = keras.layers.Dense(dim)\n",
    "        self.proj_drop = keras.layers.Dropout(proj_drop)\n",
    "\n",
    "    def call(self, x):\n",
    "        B, N, C = x.shape\n",
    "        q = self.wq(x[:, 0:1, ...]).reshape(B, 1, self.num_heads, C // self.num_heads).transpose((0, 2, 1, 3))\n",
    "        k = self.wk(x).reshape(B, N, self.num_heads, C // self.num_heads).transpose((0, 2, 1, 3))\n",
    "        v = self.wv(x).reshape(B, N, self.num_heads, C // self.num_heads).transpose((0, 2, 1, 3))\n",
    "        attn = (q @ k.transpose((0,1,3,2))) * self.scale\n",
    "        attn = tf.nn.softmax(attn, axis=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose((0,2, 1,3)).reshape(B, 1, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class CrossAttentionBlock(keras.layers.Layer):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=tf.nn.gelu, norm_layer=keras.layers.LayerNormalization, has_mlp=True):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer()\n",
    "        self.attn = CrossAttention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else Identity()\n",
    "        self.has_mlp = has_mlp\n",
    "        if has_mlp:\n",
    "            self.norm2 = norm_layer()\n",
    "            mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "            self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = x[:, 0:1, ...] + self.drop_path(self.attn(self.norm1(x)))\n",
    "        if self.has_mlp:\n",
    "            x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiScaleBlock(keras.layers.Layer):\n",
    "    def __init__(self, dim, patches, depth, num_heads, mlp_ratio, qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=tf.nn.gelu, norm_layer=keras.layers.LayerNormalization):\n",
    "        super().__init__()\n",
    "\n",
    "        num_branches = len(dim)\n",
    "        self.num_branches = num_branches\n",
    "        self.blocks = []\n",
    "        for d in range(num_branches):\n",
    "            tmp = []\n",
    "            for i in range(depth[d]):\n",
    "                tmp.append(\n",
    "                    Block(dim=dim[d], num_heads=num_heads[d], mlp_ratio=mlp_ratio[d], qkv_bias=qkv_bias,\n",
    "                          drop=drop, attn_drop=attn_drop, drop_path=drop_path[i], norm_layer=norm_layer))\n",
    "\n",
    "            if len(tmp) != 0:\n",
    "                self.blocks.append(keras.models.Sequential(tmp))\n",
    "\n",
    "        if len(self.blocks) == 0:\n",
    "            self.blocks = None\n",
    "\n",
    "        self.projs = []\n",
    "        for d in range(num_branches):\n",
    "            if dim[d] == dim[(d + 1) % num_branches] and False:\n",
    "                tmp = [Identity()]\n",
    "            else:\n",
    "                tmp = [norm_layer(), keras.layers.Lambda(act_layer), keras.layers.Dense(dim[(d + 1) % num_branches])]\n",
    "            self.projs.append(keras.models.Sequential(tmp))\n",
    "\n",
    "        self.fusion = []\n",
    "        for d in range(num_branches):\n",
    "            d_ = (d + 1) % num_branches\n",
    "            nh = num_heads[d_]\n",
    "            if depth[-1] == 0:\n",
    "                self.fusion.append(\n",
    "                    CrossAttentionBlock(dim=dim[d_], num_heads=nh, mlp_ratio=mlp_ratio[d], qkv_bias=qkv_bias,\n",
    "                                        qk_scale=qk_scale,\n",
    "                                        drop=drop, attn_drop=attn_drop, drop_path=drop_path[-1], norm_layer=norm_layer,\n",
    "                                        has_mlp=False))\n",
    "            else:\n",
    "                tmp = []\n",
    "                for _ in range(depth[-1]):\n",
    "                    tmp.append(CrossAttentionBlock(dim=dim[d_], num_heads=nh, mlp_ratio=mlp_ratio[d], qkv_bias=qkv_bias,\n",
    "                                                   qk_scale=qk_scale,\n",
    "                                                   drop=drop, attn_drop=attn_drop, drop_path=drop_path[-1],\n",
    "                                                   norm_layer=norm_layer,\n",
    "                                                   has_mlp=False))\n",
    "                self.fusion.append(keras.models.Sequential(tmp))\n",
    "\n",
    "        self.revert_projs = []\n",
    "        for d in range(num_branches):\n",
    "            if dim[(d + 1) % num_branches] == dim[d] and False:\n",
    "                tmp = [Identity()]\n",
    "            else:\n",
    "                tmp = [norm_layer(), keras.layers.Lambda(act_layer), keras.layers.Dense(dim[d])]\n",
    "            self.revert_projs.append(keras.models.Sequential(tmp))\n",
    "\n",
    "    def call(self, x):\n",
    "        outs_b = [block(x_) for x_, block in zip(x, self.blocks)]\n",
    "        proj_cls_token = [proj(x[:, 0:1]) for x, proj in zip(outs_b, self.projs)]\n",
    "        outs = []\n",
    "        for i in range(self.num_branches):\n",
    "            tmp = tf.concat([proj_cls_token[i], outs_b[(i + 1) % self.num_branches][:, 1:, ...]], axis=1)\n",
    "            tmp = self.fusion[i](tmp)\n",
    "            reverted_proj_cls_token = self.revert_projs[i](tmp[:, 0:1, ...])\n",
    "            tmp = tf.concat([reverted_proj_cls_token, outs_b[i][:, 1:, ...]], axis=1)\n",
    "            outs.append(tmp)\n",
    "\n",
    "        return outs\n",
    "\n",
    "\n",
    "def _compute_num_patches(img_size, patches):\n",
    "    return [i // p * i // p for i, p in zip(img_size, patches)]\n",
    "\n",
    "\n",
    "class VisionTransformer(keras.models.Model):\n",
    "    def __init__(self, img_size=(224, 224), patch_size=(8, 16), in_chans=3, num_classes=1000, embed_dim=(192, 384),\n",
    "                 depth=([1, 3, 1], [1, 3, 1], [1, 3, 1]),\n",
    "                 num_heads=(6, 12), mlp_ratio=(2., 2., 4.), qkv_bias=False, qk_scale=None, drop_rate=0.,\n",
    "                 attn_drop_rate=0.,\n",
    "                 drop_path_rate=0., hybrid_backbone=None, norm_layer=keras.layers.LayerNormalization, multi_conv=False):\n",
    "        super().__init__()\n",
    "        # print(\"size\",patch_size)\n",
    "        self.num_classes = num_classes\n",
    "        if not isinstance(img_size, list):\n",
    "            img_size = to_2tuple(img_size)\n",
    "        self.img_size = img_size\n",
    "\n",
    "        num_patches = _compute_num_patches(img_size, patch_size)\n",
    "        # print(\"patches\",num_patches)\n",
    "        self.num_branches = len(patch_size)\n",
    "        self.patch_embed = []\n",
    "        # if hybrid_backbone is None:\n",
    "        self.pos_embed = [tf.Variable(initial_value=tf.zeros((1,1+num_patches[i],embed_dim[i])),dtype=tf.float32) for i in range(self.num_branches) ]\n",
    "        for im_s, p, d in zip(img_size, patch_size, embed_dim):\n",
    "            self.patch_embed.append(\n",
    "                PatchEmbed(img_size=im_s, patch_size=p, in_chans=in_chans, embed_dim=d, multi_conv=multi_conv))\n",
    "\n",
    "        self.cls_token = [tf.Variable(initial_value=tf.zeros(shape=(1, 1, embed_dim[i])), trainable=True) for i in\n",
    "                          range(self.num_branches)]\n",
    "        self.pos_drop = keras.layers.Dropout(drop_rate)\n",
    "        total_depth = tf.reduce_sum([tf.reduce_sum(x[-2:]) for x in depth]).numpy()\n",
    "        dpr = [x.numpy() for x in tf.linspace(tf.constant(0.), tf.constant(drop_path_rate), tf.constant(total_depth))]\n",
    "        dpr_ptr = 0\n",
    "        self.blocks = []\n",
    "        for idx, block_cfg in enumerate(depth):\n",
    "            curr_depth = max(block_cfg[:-1]) + block_cfg[-1]\n",
    "            dpr_ = dpr[dpr_ptr:dpr_ptr + curr_depth]\n",
    "            blk = MultiScaleBlock(embed_dim, num_patches, block_cfg, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "                                  qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                                  drop_path=dpr_,\n",
    "                                  norm_layer=norm_layer)\n",
    "            dpr_ptr += curr_depth\n",
    "            self.blocks.append(blk)\n",
    "\n",
    "        self.norm = [norm_layer() for i in range(self.num_branches)]\n",
    "        self.head = [keras.layers.Dense(num_classes) if num_classes > 0 else Identity() for i\n",
    "                     in range(self.num_branches)]\n",
    "\n",
    "        # for i in range(self.num_branches):\n",
    "        #     if self.pos_embed[i].trainable:\n",
    "        #         self.pos_embed[i].assign(tf.random.truncated_normal(shape=self.pos_embed[i].shape, stddev=0.02))\n",
    "        #     self.cls_token[i].assign(tf.random.truncated_normal(shape=self.cls_token[i].shape, stddev=0.02))\n",
    "\n",
    "    def get_classifier(self):\n",
    "        return self.head\n",
    "\n",
    "    def reset_classifier(self, num_classes, global_pool=''):\n",
    "        self.num_classes = num_classes\n",
    "        self.head = keras.layers.Dense(num_classes) if num_classes > 0 else Identity()\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        xs = []\n",
    "        for i in range(self.num_branches):\n",
    "            temp_x=x.transpose((0,2,3,1))\n",
    "            # print(\"temp_x\",temp_x.shape)\n",
    "            x_ = tf.image.resize(temp_x, size=(self.img_size[i], self.img_size[i]),\n",
    "                                 method=tf.image.ResizeMethod.BICUBIC) if H != self.img_size[i] else temp_x\n",
    "            # print(\"before_x_\",x_.shape)\n",
    "            x_=x_.transpose((0,3,1,2))\n",
    "            # print(\"x_\",x_.shape)\n",
    "            tmp = self.patch_embed[i](x_)\n",
    "            cls_tokens = tf.experimental.numpy.tile(self.cls_token[i], (B, 1, 1))\n",
    "            # print(\"tokens\",cls_tokens.shape)\n",
    "            tmp = tf.concat([cls_tokens, tmp], axis=1)\n",
    "            # print(\"tmp\",tmp.shape)\n",
    "            # print(\"pos\",self.pos_embed[i].shape)\n",
    "            tmp2=tmp+self.pos_embed[i]\n",
    "            tmp2 = self.pos_drop(tmp2)\n",
    "            xs.append(tmp2)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            xs = blk(xs)\n",
    "\n",
    "        xs = [self.norm[i](x) for i, x in enumerate(xs)]\n",
    "        out = [x[:, 0] for x in xs]\n",
    "\n",
    "        return out\n",
    "\n",
    "    def call(self, x):\n",
    "        xs = self.forward_features(x)\n",
    "        ce_logits = [self.head[i](x) for i, x in enumerate(xs)]\n",
    "        ce_logits = tf.reduce_mean(tf.stack(ce_logits, axis=0), axis=0)\n",
    "\n",
    "        return ce_logits\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def crossvit_tiny_224(pretrained=False, **kwargs):\n",
    "    model = VisionTransformer(img_size=[240, 224],\n",
    "                              patch_size=[12, 16], embed_dim=[96, 192], depth=[[1, 4, 0], [1, 4, 0], [1, 4, 0]],\n",
    "                              num_heads=[3, 3], mlp_ratio=[4, 4, 1], qkv_bias=True,\n",
    "                              norm_layer=keras.layers.LayerNormalization, **kwargs)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "model=crossvit_tiny_224()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "model.build([32,3, 224,224])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vision_transformer_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " patch_embed_6 (PatchEmbed)  multiple                  41568     \n",
      "                                                                 \n",
      " patch_embed_7 (PatchEmbed)  multiple                  147648    \n",
      "                                                                 \n",
      " dropout_219 (Dropout)       multiple                  0         \n",
      "                                                                 \n",
      " multi_scale_block_9 (MultiS  multiple                 2152800   \n",
      " caleBlock)                                                      \n",
      "                                                                 \n",
      " multi_scale_block_10 (Multi  multiple                 2152800   \n",
      " ScaleBlock)                                                     \n",
      "                                                                 \n",
      " multi_scale_block_11 (Multi  multiple                 2152800   \n",
      " ScaleBlock)                                                     \n",
      "                                                                 \n",
      " layer_normalization_198 (La  multiple                 192       \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " layer_normalization_199 (La  multiple                 384       \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_390 (Dense)           multiple                  97000     \n",
      "                                                                 \n",
      " dense_391 (Dense)           multiple                  193000    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,014,800\n",
      "Trainable params: 7,014,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "ename": "UnimplementedError",
     "evalue": "Exception encountered when calling layer \"conv2d_12\" (type Conv2D).\n\nThe Conv2D op currently only supports the NHWC tensor format on the CPU. The op was given the format: NCHW [Op:Conv2D]\n\nCall arguments received by layer \"conv2d_12\" (type Conv2D):\n  • inputs=tf.Tensor(shape=(32, 3, 240, 240), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnimplementedError\u001B[0m                        Traceback (most recent call last)",
      "Input \u001B[1;32mIn [34]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m ins\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mnormal(shape\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m32\u001B[39m,\u001B[38;5;241m3\u001B[39m,\u001B[38;5;241m224\u001B[39m,\u001B[38;5;241m224\u001B[39m))\n\u001B[1;32m----> 2\u001B[0m outs\u001B[38;5;241m=\u001B[39m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mins\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\ML\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[0;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m---> 67\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     69\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "Input \u001B[1;32mIn [30]\u001B[0m, in \u001B[0;36mVisionTransformer.call\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    275\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcall\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m--> 276\u001B[0m     xs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    277\u001B[0m     ce_logits \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhead[i](x) \u001B[38;5;28;01mfor\u001B[39;00m i, x \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(xs)]\n\u001B[0;32m    278\u001B[0m     ce_logits \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mreduce_mean(tf\u001B[38;5;241m.\u001B[39mstack(ce_logits, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m), axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "Input \u001B[1;32mIn [30]\u001B[0m, in \u001B[0;36mVisionTransformer.forward_features\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    255\u001B[0m x_\u001B[38;5;241m=\u001B[39mx_\u001B[38;5;241m.\u001B[39mtranspose((\u001B[38;5;241m0\u001B[39m,\u001B[38;5;241m3\u001B[39m,\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m2\u001B[39m))\n\u001B[0;32m    256\u001B[0m \u001B[38;5;66;03m# print(\"x_\",x_.shape)\u001B[39;00m\n\u001B[1;32m--> 257\u001B[0m tmp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpatch_embed\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    258\u001B[0m cls_tokens \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mnumpy\u001B[38;5;241m.\u001B[39mtile(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcls_token[i], (B, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m))\n\u001B[0;32m    259\u001B[0m \u001B[38;5;66;03m# print(\"tokens\",cls_tokens.shape)\u001B[39;00m\n",
      "Input \u001B[1;32mIn [30]\u001B[0m, in \u001B[0;36mPatchEmbed.call\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     48\u001B[0m B, C, H, W \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mshape\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m H \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimg_size[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;129;01mand\u001B[39;00m W \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimg_size[\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;241m1\u001B[39m], \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInput image size (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mH\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m*\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mW\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) doesn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt match model (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimg_size[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m*\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimg_size[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m).\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m---> 51\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mproj\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     52\u001B[0m x \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mreshape(x,shape\u001B[38;5;241m=\u001B[39m(x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m],x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m],\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\n\u001B[0;32m     53\u001B[0m x\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mnumpy\u001B[38;5;241m.\u001B[39mswapaxes(x,\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m2\u001B[39m)\n",
      "\u001B[1;31mUnimplementedError\u001B[0m: Exception encountered when calling layer \"conv2d_12\" (type Conv2D).\n\nThe Conv2D op currently only supports the NHWC tensor format on the CPU. The op was given the format: NCHW [Op:Conv2D]\n\nCall arguments received by layer \"conv2d_12\" (type Conv2D):\n  • inputs=tf.Tensor(shape=(32, 3, 240, 240), dtype=float32)"
     ]
    }
   ],
   "source": [
    "ins=tf.random.normal(shape=(32,3,224,224))\n",
    "outs=model(ins)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}